# enable robot to imagine

- world model
- VAE
- Latent State Space Forward Dynamics
- Behavior Controller


## KL Divergence
[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is a type of statistical distance: a measure of how one probability distribution P is different from a second probability distribution Q. A simple interpretation of the KL divergence of P from Q is the expected excess surprise from using Q as a model when the actual distribution is P. A way to measure dissimilarity between two probability distributions based on information theory considering entropy H(x) = - SUM_x(P(x)logP(x)). KL divergence (always positive) could be interpreated as number of bits of information lost if we use distribution Q to represent P, or negative log likelihood that samples generated by distribution P has been generated by Q.

The KL divergence between two normal distributions P (u1,s1) and Q (u2,s1) is
KL(P,Q) = log(s2/s1) + (s1^2+(u1-u2)^2)/2s2^2 -1/2 

## Reference
-[A Beginner's Guide to Variational Methods: Mean-Field Approximation](https://blog.evjang.com/2016/08/variational-bayes.html)
-[CMU - Variational Autoencoder](http://www.cs.cmu.edu/afs/cs/user/bhiksha/WWW/courses/deeplearning/Fall.2018/www/recitations/rec10.vae.pdf)
-[The KL divergence bwteen continuous probability distributions](https://blogs.sas.com/content/iml/2020/06/01/the-kullback-leibler-divergence-between-continuous-probability-distributions.html)
