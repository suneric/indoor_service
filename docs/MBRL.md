# Model-Based Reinforcement Learning

Typical deep RL algorithms require tremendous training samples, resulting in much high sample complexity, thus are hard to be directly applied in real-world tasks, where trial-and-error can be highly costly. Model-based reinforcement learning (MBRL) is on of the most important method to improve sample efficiency, and it is believed to have the great potential to make RL algorithms significantly more sample efficiency [1]. In MBRL, the environment model refers to the abstraction of the environment dynamics with the learning agents interacts. **Learning the model corresponds to recovering the state transition dynamics M and the reward function R**, in a environment formulated as a MDP <S,A,M,R,$$\gamma$$>.   
With an environment model, the agent can have the imagination ability. It can interact with the model to sample the interaction data, which is **simulation data**. Compared to the model-free reinforcement learning (MFRL) methods, where the agent can only use the data sampled from the interaction with the real environment, called **experienced data**.

## Enable robot to imagine
- world model
- VAE
- Latent State Space Forward Dynamics
- Behavior Controller

## Model Learning

In MDP <S,A,M,R,$$\gamma$$>, the state transition dynamics M and the reward function R are to be learned.

### Model learning in tabular settings
At the early stage of RL research, the state and actions spaces are finite and small, the model learning is considered with the tabular MDPs.

### Model learning via prediction loss
For large-scale MDPs and MDPs with continuous state space and action space, Approximation functions are therefore employed in the general setting.

### Model learning with reduced error
To solve the major issue (horizon-qaured compounding error) due to the use of prediction loss to learn an unconstrained model.

### Model learning for complex environments dynamics
The mainstream realization of the environment dynamics model is an ensemble of Gaussian processes where the mean vector and covariance matrix for the distribution of the next state are built based on neural networks fed in the current state-action pair.
- Partial Observability, partial observable MDP, observation model p(o_t|s_t) and a latent transition model p(s_{t+1}|s_t,a_t) are learned via maximizing a posterior and the posterior distribution p(s_t|o_1,...o_t) can be inferred.
- **Representation learning**, for high-dimension state space such as images, representation learning that learns informative latent state or action representation will much benefit the environment model building so as to improve the effectiveness and sample efficiency of model-based RL.

## Likelihood
Suppose a stochastic process (s) and we can calculate the probability of observing (O) a particular set of outcomes by making suitable assumptions about it.
The probability we want to calculate is P(O|s), in other words, given specific s, P(O|s) is the probability that we would observe the outcomes represented by O. However, when we model a real life stochastic process, we often do not know s, We simply observe O and the goal is to arrive at an estimate for s that world be a plausible choice given the observed O. We know that given a value of s the probability of observing O is P(O|s). Thus a 'natural' estimation process is to choose that the value of s that would maximize the probability that we would actually observed O. In other words, we find the parameter values s that maximize the following function: L(s|O) = P(O|s), L(s|O) is called the likelihood function, Notice that by definition the likelihood function is conditioned on the observed O and it is a function of the unknown parameters s. In practice, log probability is used instead of probability.

## KL Divergence
[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is a type of statistical distance: a measure of how one probability distribution P is different from a second probability distribution Q. A simple interpretation of the KL divergence of P from Q is the expected excess surprise from using Q as a model when the actual distribution is P. A way to measure dissimilarity between two probability distributions based on information theory considering entropy H(x) = - SUM_x(P(x)logP(x)). KL divergence (always positive) could be interpreted as number of bits of information lost if we use distribution Q to represent P, or negative log likelihood that samples generated by distribution P has been generated by Q.

The KL divergence between two normal distributions P (u1,s1) and Q (u2,s2) is
KL(P,Q) = log(s2/s1) + (s1^2+(u1-u2)^2)/2s2^2 -1/2

## Reference
[1] A survey on model-based reinforcement learning
-[A Beginner's Guide to Variational Methods: Mean-Field Approximation](https://blog.evjang.com/2016/08/variational-bayes.html)
-[CMU - Variational Autoencoder](http://www.cs.cmu.edu/afs/cs/user/bhiksha/WWW/courses/deeplearning/Fall.2018/www/recitations/rec10.vae.pdf)
-[The KL divergence bwteen continuous probability distributions](https://blogs.sas.com/content/iml/2020/06/01/the-kullback-leibler-divergence-between-continuous-probability-distributions.html)
